
Train_model = function(TEST = FALSE, limiting_n_observations = 100){

library(glmnet)
library(Matrix)

  get_CRAN_logs_output = CTVsuggest:::get_CRAN_logs(TEST = TEST, limiting_n_observations = limiting_n_observations)


#### Objects needed to run generated by previous script ####
# tvdb
# no_tsk_pckgs_meet_threshold
# final_package_names
# features
# response_matrix
# all_CRAN_pks

##### Objects Outputted ####
# predicted_probs_for_suggestions


# setwd("~/Documents/Projects/CRAN-Task-Views-Recommendations")
#save(list = ls(),file = "objects_saved_before_Train_model_script.rda")
#load(file = "objects_saved_before_Train_model_script.rda")
current_objects = ls()



#### Pins code ####
#library(pins)
#board = board_folder(path = "Pins_board/")

# features                    = board %>% pin_read("features")
# response_matrix             = board %>% pin_read("response_matrix")
# final_package_names         = board %>% pin_read("final_package_names")
# tvdb                        = board %>% pin_read("tvdb")
########

# Loading vector of packages with no Task View assignment that do not meet threshold
# no_tsk_pckgs_meet_threshold = board %>% pin_read("no_tsk_pckgs_meet_threshold")
no_tsk_pckgs_meet_threshold = base::intersect(no_tsk_pckgs_meet_threshold, final_package_names)
no_tsk_pckgs_meet_threshold = unique(no_tsk_pckgs_meet_threshold)

# The training and testing sets are made up of the packages that either have:
# no Task View but meet download threshold
# (response_matrix[no_tsk_pckgs_meet_threshold,])
# or has an assigned Task View
# (response_matrix[response_matrix[,"none"] == 0,])

# combining the two sets
labelled_data_res = (rbind(response_matrix[response_matrix[,"none"] == 0,],    response_matrix[no_tsk_pckgs_meet_threshold,]))
labelled_data_features = features[rownames(labelled_data_res),]


# labelled_data_res_df = rbind(response_df[!(response_df[,"TaskViews"] == "none"),], response_df[no_tsk_pckgs_meet_threshold,])

set.seed(3)
split1<- sample(c(rep(0, 0.8 * nrow(labelled_data_res)), rep(1, 0.2 * nrow(labelled_data_res))))
table(split1)
train_res = labelled_data_res[split1 == 0,]
train_features = labelled_data_features[split1 == 0,]
test_res = labelled_data_res[split1 == 1,]
#test_res_df = labelled_data_res_df[split1 == 1,]
test_feature = labelled_data_features[split1 == 1,]


#### fitting multinomial model ####
# library(nnet)
#
# model_multinom = multinom(train_res ~ ., data = train_features, MaxNWts=10000)


##### LASSO #####
library(glmnet)

# removing row that has missing features
# train_res = train_res[!apply(as.matrix(train_features),1, function(x){any(is.na(x))}),]
# train_features = train_features[!apply(as.matrix(train_features),1, function(x){any(is.na(x))}),]

train_res = as.matrix(train_res)
train_features = as.matrix(train_features)


library(Matrix)

train_sparse <- sparse.model.matrix(~., as.data.frame(train_features))
train_res_sparse <- sparse.model.matrix(~0 + ., as.data.frame(train_res))

message("Training model")
set.seed(3)
model_multinom_cv = cv.glmnet(x = train_sparse,  y = train_res, family = "multinomial", alpha = 1, trace.it = 1,  maxit = 1e+06)
# board %>% pin_write(model_multinom_cv, "model_multinom_cv", type = "rds")



# model_multinom_cv = board %>% pin_read("model_multinom_cv")




#### XGboost ####
# to use XGboost we need to put the data into another format
# I am following the example given here:
# https://rstudio-pubs-static.s3.amazonaws.com/456044_9c275b0718a64e6286751bb7c60ae42a.html

# Instead of response matrix need to have vector of labels
# Also XGboost will not deal with multiple labels in the same way cv.glmnet does
# need to create seperate row for each different label

# head(train_features)
# head(train_res)
#
# head(labelled_data_features)
#
# label_XGb = c()
# for(i in 1:nrow(labelled_data_res)){
#   # i = 33
#   # i = 34
#   x = labelled_data_res[i,,drop = FALSE]
#   y = colnames(labelled_data_res)[which(x == 1)]
#   names(y) = rep(row.names(x), length(y))
#
#   label_XGb = c(label_XGb,y)
#
# }
#
# label_XGb_integer = as.integer(factor(label_XGb)) - 1
#
# labelled_data_features_xGb = labelled_data_features
# labelled_data_features_xGb = labelled_data_features_xGb[names(label_XGb),]
#
#
# set.seed(3)
# split1<- sample(c(rep(0, 0.8 * nrow(labelled_data_features_xGb)), rep(1, 0.2 * nrow(labelled_data_features_xGb))))
# train_res_xGB = label_XGb_integer[split1 == 0]
# train_features_xGB = labelled_data_features_xGb[split1 == 0,]
# test_res_xGB = label_XGb_integer[split1 == 1]
# #test_res_df = labelled_data_res_df[split1 == 1,]
# test_feature_xGB = labelled_data_features_xGb[split1 == 1,]
#
#
# # train_features_XGb = train_features_XGb[names(label_XGb),]
# # test_features_XGb = test_features_XGb[names(label_XGb),]
#
# library(xgboost)
# xgb.train = xgb.DMatrix(data=as.matrix(train_features_xGB),label=train_res_xGB)
# xgb.test = xgb.DMatrix(data=as.matrix(test_feature_xGB), label=test_res_xGB)
#
#
# num_class = length(levels(factor(label_XGb)))
# params = list(
#   booster="gbtree",
#   eta=0.001,
#   max_depth=20,
#   gamma=3,
#   subsample=0.75,
#   colsample_bytree=1,
#   objective="multi:softprob",
#   eval_metric="mlogloss",
#   num_class=num_class
# )
#
# xgb.fit=xgb.train(
#   params=params,
#   data=xgb.train,
#   nrounds=100,
#   nthreads=8,
#   early_stopping_rounds=10,
#   watchlist=list(val1=xgb.train,val2=xgb.test),
#   verbose=0
# )
#
# xgb.pred = predict(xgb.fit,as.matrix(test_feature_xGB),reshape=T)
# xgb.pred = as.data.frame(xgb.pred)
# colnames(xgb.pred) = levels(factor(label_XGb))
#
# xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
# xgb.pred$label = levels(factor(label_XGb))[test_res_xGB+1]
#
# # Calculate the final accuracy
# result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
# print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
#
#
#


#### Trying to convert into tidymodel form
# library(tidymodels)
#
# head(labelled_data_features_xGb)
# head(label_XGb)
#
# tidymodel_multinom_data = labelled_data_features_xGb
# tidymodel_multinom_data$TaskView = factor(label_XGb)
# tidymodel_multinom_data = as_tibble(tidymodel_multinom_data)
#
# set.seed(3)
# split1<- sample(c(rep(0, 0.8 * nrow(tidymodel_multinom_data)), rep(1, 0.2 * nrow(tidymodel_multinom_data))))
# tidymodel_multinom_data_train = tidymodel_multinom_data[split1 == 0,]
# tidymodel_multinom_data_test = tidymodel_multinom_data[split1 == 1,]
#
# mr_cls_fit <- mr_cls_spec %>% fit(TaskView ~ ., data = tidymodel_multinom_data_train)
# mr_cls_fit
#
# predict(mr_cls_fit, tidymodel_multinom_data_test)
#
# library(vetiver)
# v = vetiver_model(mr_cls_fit, "mr_cls_fit")
#
# multinom_reg(mode = "classification",
#              engine = "glmnet",
#              penalty = model_multinom_cv$lambda.min,
#              mixture = 1) %>%
#   fit(train_res ~ ., data = train_features)
#
#
#

#### Trying tidymodels workflow ####
# This example shows how the data should be formatted: https://parsnip.tidymodels.org/articles/Examples.html#multinom_reg-models
# This page goes through some more technical aspects of using the glmnet engine with tidymodels: https://parsnip.tidymodels.org/reference/glmnet-details.html
# I used the code from: https://stackoverflow.com/questions/71397075/why-does-deploying-a-tidymodel-with-vetiver-throw-a-error-when-theres-a-variabl
# tskviews_recipe <-
#   recipe(formula = TaskView ~ ., data = tidymodel_multinom_data_train)
#
# rf_spec <- multinom_reg(mode = "classification", penalty = model_multinom_cv$lambda.min,
#                         mixture = 1) %>%
#                         set_engine("glmnet", path_values = model_multinom_cv$lambda.min)
#
# rf_fit <-
#   workflow() %>%
#   add_model(rf_spec) %>%
#   add_recipe(tskviews_recipe) %>%
#   fit(tidymodel_multinom_data_train)
#
# mean(as.data.frame(predict(rf_fit, tidymodel_multinom_data_test[,-ncol(tidymodel_multinom_data_test)]))[,1] == tidymodel_multinom_data_test$TaskView)
#
# library(vetiver)
# v <- vetiver_model(rf_fit, "tsk_views_tidymodel")


#### Accuracy ####


model = model_multinom_cv
predict_class = predict(model, newx = cbind(rep(1, nrow(test_feature)),as.matrix(test_feature)), s = "lambda.min",  type = "class")
# Getting accuracy of model after applying lasso with min Lambda
predict_class = factor(predict_class[,1], levels = c(tvdb_vec(), "none"))

# model = model_multinom
# predict_class = predict(model, newx = cbind(rep(1, nrow(test_feature)),as.matrix(test_feature)),  type = "class")
# predict_class = factor(predict_class, levels = c(tvdb_vec(), "none"))



model_accuracy = mean(test_res[cbind(1:nrow(test_res), predict_class)], na.rm = T)
model_accuracy = 100*model_accuracy

apply(test_res[which(test_res[cbind(1:nrow(test_res), predict_class)] == 0),],2,sum)



##### top 3 recommendations from model ######

#
# predict_class = predict(model, newx = cbind(rep(1, nrow(test_feature)),as.matrix(test_feature)), s = "lambda.min",  type = "class")
# predict_prob = predict(model, newx = cbind(rep(1, nrow(test_feature)),as.matrix(test_feature)), s = "lambda.min", type = "response")



#load(file = paste0("Code/Multinomial_models/Predictors/",date,"/no_taskview_pckgs_that_meet_threshold.RData"))





pkgs_for_suggestions = all_CRAN_pks[!(all_CRAN_pks %in% no_tsk_pckgs_meet_threshold) & (response_matrix[,"none"] == 1)]
pkgs_for_suggestions_features = features[pkgs_for_suggestions,]

# If there are packages that belong to `pkgs_for_suggestions` but are not in the rownames of features then creates some NA rows. Delete these here:
pkgs_for_suggestions_features = pkgs_for_suggestions_features[!apply(as.matrix(pkgs_for_suggestions_features),1, function(x){any(is.na(x))}),]

# board %>% pin_write(pkgs_for_suggestions_features, "pkgs_for_suggestions_features")


predicted_probs_for_suggestions = predict(model_multinom_cv, newx = cbind(rep(1, nrow(pkgs_for_suggestions_features)),as.matrix(pkgs_for_suggestions_features)), s = "lambda.min", type = "response")
predicted_probs_for_suggestions = data.frame(predicted_probs_for_suggestions)
colnames(predicted_probs_for_suggestions) = gsub(x = colnames(predicted_probs_for_suggestions), pattern = "\\.1", "")
predicted_probs_for_suggestions$Packages = row.names(predicted_probs_for_suggestions)

save(predicted_probs_for_suggestions, file = "Output/predicted_probs_for_suggestions.rda")
save(model_accuracy, file = "Output/model_accuracy.rda")


}



# board =       legacy_github(
#               repo = "DylanDijk/CRAN-Task-Views-Recommendations"
#             )
#
# p = predicted_probs_for_suggestions
# board %>% pin(p, "p")
# board %>% pin_write(predicted_probs_for_suggestions, "predicted_probs_for_suggestions")







# predicted_probs_for_suggestions = board %>% pin_read("predicted_probs_for_suggestions")




# suggestions_for_Task_View = function(TaskView = "Hydrology", n = 5){
#
#   suggestions = row.names(predicted_probs_for_suggestions[,paste0(TaskView,".1"), drop = F][order(predicted_probs_for_suggestions[,paste0(TaskView,".1"), drop = F], decreasing = T),, drop = F])[1:n]
#   return(suggestions)
# }
#
#
# suggestions_for_Task_View(TaskView = "SportsAnalytics")
# suggestions_for_Task_View(TaskView = "NaturalLanguageProcessing")
# predicted_probs_for_suggestions["multxpert",]




#### Vetiver ####
# vetiver does not accept the cv.glmnet model type
# model_multinom_cv = board %>% pin_read("model_multinom_cv")
# library(tidymodels)
# tidy_model_multinom_cv = broom::tidy(model_multinom_cv)
#
# library(vetiver)
# v <- vetiver_model(tidy_model_multinom_cv, "model_multinom_cv")
