#' Title
#'
#' @param TEST
#' @param limiting_n_observations
#' @param save logical. If TRUE then saves objects that are returned by this function to disk.
#'
#' @return
#' @export
#'
#' @examples
Train_model = function(TEST = FALSE, limiting_n_observations = 100, save = FALSE){


#### Objects needed to run generated by previous script ####
# tvdb
# no_tsk_pckgs_meet_threshold
# final_package_names
# features
# response_matrix
# all_CRAN_pks

##### Objects Outputted ####
# predicted_probs_for_suggestions



#### ----------------------------------------------------------------------------------------------- ####

get_CRAN_logs_output = CTVsuggest:::get_CRAN_logs(TEST = TEST, limiting_n_observations = limiting_n_observations)

#### ----------------------------------------------------------------------------------------------- ####




#### ----------------------------------------------------------------------------------------------- ####

# Loading vector of packages with no Task View assignment that do not meet threshold
no_tsk_pckgs_meet_threshold = base::intersect(get_CRAN_logs_output$no_tsk_pckgs_meet_threshold, get_CRAN_logs_output$final_package_names)
no_tsk_pckgs_meet_threshold = unique(no_tsk_pckgs_meet_threshold)

# The training and testing sets are made up of the packages that either have:
# no Task View but meet download threshold
# (response_matrix[no_tsk_pckgs_meet_threshold,])
# or has an assigned Task View
# (response_matrix[response_matrix[,"none"] == 0,])

# combining the two sets
labelled_data_res = (rbind(get_CRAN_logs_output$response_matrix[get_CRAN_logs_output$response_matrix[,"none"] == 0,],    get_CRAN_logs_output$response_matrix[no_tsk_pckgs_meet_threshold,]))
labelled_data_features = get_CRAN_logs_output$features[rownames(labelled_data_res),]


# labelled_data_res_df = rbind(response_df[!(response_df[,"TaskViews"] == "none"),], response_df[no_tsk_pckgs_meet_threshold,])

set.seed(3)
split1<- sample(c(rep(0, 0.8 * nrow(labelled_data_res)), rep(1, 0.2 * nrow(labelled_data_res))))
table(split1)
train_res = labelled_data_res[split1 == 0,]
train_features = labelled_data_features[split1 == 0,]
test_res = labelled_data_res[split1 == 1,]
#test_res_df = labelled_data_res_df[split1 == 1,]
test_feature = labelled_data_features[split1 == 1,]

#### ----------------------------------------------------------------------------------------------- ####




#### ----------------------------------------------------------------------------------------------- ####
##### LASSO #####

# removing row that has missing features
# train_res = train_res[!apply(as.matrix(train_features),1, function(x){any(is.na(x))}),]
# train_features = train_features[!apply(as.matrix(train_features),1, function(x){any(is.na(x))}),]

train_res = as.matrix(train_res)
train_features = as.matrix(train_features)



train_sparse <- Matrix::sparse.model.matrix(~., as.data.frame(train_features))
train_res_sparse <- Matrix::sparse.model.matrix(~0 + ., as.data.frame(train_res))

message("Training model")
set.seed(3)
model_multinom_cv = glmnet::cv.glmnet(x = train_sparse,  y = train_res, family = "multinomial", alpha = 1, trace.it = 1,  maxit = 1e+06)
#### ----------------------------------------------------------------------------------------------- ####



#### ----------------------------------------------------------------------------------------------- ####
#### Accuracy ####

model = model_multinom_cv
predict_class = predict(model, newx = cbind(rep(1, nrow(test_feature)),as.matrix(test_feature)), s = "lambda.min",  type = "class")
# Getting accuracy of model after applying lasso with min Lambda
predict_class = factor(predict_class[,1], levels = c(RWsearch::tvdb_vec(get_CRAN_logs_output$tvdb), "none"))



model_accuracy = mean(test_res[cbind(1:nrow(test_res), predict_class)], na.rm = T)
model_accuracy = 100*model_accuracy

# apply(test_res[which(test_res[cbind(1:nrow(test_res), predict_class)] == 0),],2,sum)
#### ----------------------------------------------------------------------------------------------- ####


#### ----------------------------------------------------------------------------------------------- ####
##### Predicted probabilities form model ######
# Vector of packages that are not assigned a Task View that do not meet threshold
# Will then get classification probabilities for these packages using the trained model
pkgs_for_suggestions = get_CRAN_logs_output$final_package_names[!(get_CRAN_logs_output$final_package_names %in% get_CRAN_logs_output$no_tsk_pckgs_meet_threshold) & (get_CRAN_logs_output$response_matrix[,"none"] == 1)]
pkgs_for_suggestions_features = get_CRAN_logs_output$features[pkgs_for_suggestions,]

# If there are packages that belong to `pkgs_for_suggestions` but are not in the rownames of features then creates some NA rows. Delete these here:
pkgs_for_suggestions_features = pkgs_for_suggestions_features[!apply(as.matrix(pkgs_for_suggestions_features),1, function(x){any(is.na(x))}),]



predicted_probs_for_suggestions = predict(model_multinom_cv, newx = cbind(rep(1, nrow(pkgs_for_suggestions_features)),as.matrix(pkgs_for_suggestions_features)), s = "lambda.min", type = "response")
predicted_probs_for_suggestions = data.frame(predicted_probs_for_suggestions)
colnames(predicted_probs_for_suggestions) = gsub(x = colnames(predicted_probs_for_suggestions), pattern = "\\.1", "")
predicted_probs_for_suggestions$Packages = row.names(predicted_probs_for_suggestions)
#### ----------------------------------------------------------------------------------------------- ####

if(save){
save(model, file = "OUTPUT/predicted_probs_for_suggestions.rda")
save(predicted_probs_for_suggestions, file = "OUTPUT/predicted_probs_for_suggestions.rda")
save(model_accuracy, file = "OUTPUT/model_accuracy.rda")

}else{
    return(list("model" = model,
                "predicted_probs_for_suggestions" = predicted_probs_for_suggestions,
                "model_accuracy" = model_accuracy))
  }

}

