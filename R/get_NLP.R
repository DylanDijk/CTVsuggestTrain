#' Creates the NLP features for the model using Task View text and Package description text
#'
#' Reads the Task View text from the source markdown files on GitHub, then cleans them.
#' Then with the corpus of words generated from the Task Views text, it creates TF_IDF vectors for each of the words.
#'
#' @param TEST logical. Default is [`FALSE`]. If [`TRUE`], then a subset of the data that is extracted from CRAN is selected. This is to speed up testing.
#'
#' More precisely, if [`TRUE`] a random selection of rows from `CRAN_data` is selected, where the number of rows
#' is given by `limiting_n_observations`.
#'
#' @param limiting_n_observations Integer that decides the size of the subset of `CRAN_data`, when `TEST` is [`TRUE`].
#'
#'
#' @param get_input_stored logical. If [`TRUE`] then the function uses pre saved data as input, otherwise it runs the `CTVsuggestTrain `internal [get_data()] function.
#' @param get_input_path string. If `get_input_stored` is set to [`TRUE`], `get_input_path` gives the path loaction of the pre saved data.
#'
#'
#'
#' @param save_output logical. Default is [`FALSE`]. If [`TRUE`], then the list that is returned is saved to the path set by
#' `save_path`.
#' @param save_path string. Sets the path where the list created by the function will be saved,
#' which is when `save_output` is set to [`TRUE`]
#' @param file_name string. Sets the file name for the saved object.
#'
#' @return Returns
#'\itemize{
#'   \item feature_matrix_titles_descriptions_packages_cosine - list.
#'   With element for each package being a vector of length of the number of task views.
#'   Each element of the vector is generated by taking the cosine similarity of the TF_IDF vector of the corresponding Task View with the package text data TF_IDF
#'   vector. The IDF term for the TF_IDF vector of the package text is generated by the Task View text corpus.
#'   \item input_CRAN_data - This just a list containing all of the data created by the CTVsuggest:::get_data function, so that it is carried forward.
#' }
#'

get_NLP = function(TEST = FALSE, limiting_n_observations = 100,
                   get_input_stored = FALSE, get_input_path = "tests/testthat/fixtures/get_data_output/get_data_output.rds",
                   save_output = FALSE, save_path = "tests/testthat/fixtures/get_NLP_output", file_name){


  #### Objects needed to run generated by CTVsuggest:::get_data() ####
  # tvdb

  #### Objects Outputted ####
  # feature_matrix_titles_descriptions_packages_cosine




#### ----------------------------------------------------------------------------------------------- ####
# Getting data
# For testing may want to use prestored input object
  if(get_input_stored){
    input_CRAN_data = readRDS(get_input_path)

    }else{

# Get required objects from CTVsuggest:::get_data()
  input_CRAN_data = CTVsuggestTrain:::get_data(TEST = TEST, limiting_n_observations = limiting_n_observations)

  }

  # This script creates the NLP features for the model using Task View text and Package description text.
  message("Creating the NLP features for the model using Task View text and Package description text")

#### ----------------------------------------------------------------------------------------------- ####




#### ----------------------------------------------------------------------------------------------- ####

  ##### Reading and Cleaning Text for all Task Views  #####
  message("Reading and Cleaning Text for all Task Views ")
  TaskViews = RWsearch::tvdb_vec(input_CRAN_data$tvdb)

  TaskView_sources_text = vector(length = length(TaskViews), mode = "list")
  names(TaskView_sources_text) = TaskViews

  for(TaskView in TaskViews){
    print(TaskView)
    #TaskView = "WebTechnologies"
    #TaskView = "Databases"
    #TaskView = "OfficialStatistics"
    #TaskView = "MachineLearning"
    #TaskView = "HighPerformanceComputing"
    #TaskView = "ReproducibleResearch"

    if(TaskView == "HighPerformanceComputing"){
      raw = readLines(paste0("https://raw.githubusercontent.com/cran-task-views/",TaskView,"/master/",TaskView,".md"))
    } else{
      raw = readLines(paste0("https://raw.githubusercontent.com/cran-task-views/",TaskView,"/main/",TaskView,".md"))
    }

    YAML = raw[(min(which(raw == "---")) + 1):(max(which(raw == "---")) - 1)]
    #TaskView_references = gsub(x = raw_cln, pattern = "`r [^`]*`", " ")
    raw_cln =  raw[(max(which(raw == "---")) + 1):length(raw)]

    if(any(grepl(pattern = "# Links", x = raw_cln))){
      raw_cln =  raw_cln[1:(which(grepl(pattern = "# Links", x = raw_cln)) - 1)]
    }

    sections = raw_cln[grepl(x = raw_cln , pattern = "#")]

    if(TaskView == "ReproducibleResearch"){
      positions_of_sections_ReproducibleResearch = which(grepl(x = raw_cln , pattern = "===") | grepl(x = raw_cln , pattern = "---"))
      positions_of_sections_ReproducibleResearch = positions_of_sections_ReproducibleResearch - 1

      sections = raw_cln[positions_of_sections_ReproducibleResearch]
    }

    raw_cln = gsub(x = raw_cln, pattern = "`r[^`]*`", " ")
    raw_cln = gsub(x = raw_cln, pattern = "`[^`]*`", " ")

    raw_cln = gsub(x = raw_cln, pattern = "\\(https:[^()]*\\)", "")
    raw_cln = gsub(x = raw_cln, pattern = "\\(http:[^()]*\\)", "")
    raw_cln = gsub(x = raw_cln, pattern = "\\(http:.*", "")
    raw_cln = gsub(x = raw_cln, pattern = "\\(https:.*", "")
    raw_cln = gsub(x = raw_cln, pattern = "\\([^()]*\\)", "")

    # Some Task View text does not have separate topics for example the MachineLearning Task View
    if(any(grepl(pattern = "#", x = raw_cln))){
      intro = raw_cln[1:(min(which(grepl(raw_cln, pattern ="#"))) - 1)]
    } else {
      intro = raw_cln[1:(min(which(grepl(raw_cln, pattern ="\\*"))) - 1)]
    }

    intro = stringr::str_squish(intro)
    intro = paste(intro, collapse = " ")

    raw_cln = raw_cln[!grepl(x = raw_cln , pattern = "##")]

    raw_cln = stringr::str_squish(raw_cln)

    raw_cln = paste(raw_cln, collapse = " ")
    raw_cln = gsub(x = raw_cln, pattern = "\\([^()]*\\)", "")


    TaskView_sources_text[[TaskView]] = list(YAML = YAML, sections = sections, intro = intro, clean = raw_cln)


  }

#### ----------------------------------------------------------------------------------------------- ####



#### ----------------------------------------------------------------------------------------------- ####
  ##### Creates data frame object that gives the count of each word in each Task View ######

  n = vector(length = length(TaskViews))
  names(n) = TaskViews

  first = TaskViews[1]

  for(i in TaskViews){
    # i= TaskViews[1]
    # i= TaskViews[2]
    text = paste(TaskView_sources_text[[i]]$clean, collapse = " ")
    TaskViews_txt = dplyr::tibble(txt = text)
    # unnest_tokens converts the character string to separate words
    TaskViews_txt = tidytext::unnest_tokens(TaskViews_txt, word, txt)
    # absorbs words into lemma word
    TaskViews_txt$word = textstem::lemmatize_words(TaskViews_txt$word)
    # remove numbers
    TaskViews_txt = suppressWarnings(data.frame(word = TaskViews_txt$word[is.na(as.numeric(TaskViews_txt$word))]))
    # count number of times each word appears
    TaskViews_txt = dplyr::count(TaskViews_txt, word, sort = TRUE, name = i)

    # this is the number of unique words in each document
    n[i] = nrow(TaskViews_txt)

    if(i == first){

      corpus_word_matrix = TaskViews_txt

    }else{

      corpus_word_matrix = merge(x = corpus_word_matrix, y = TaskViews_txt, by = "word", all = TRUE)

    }

  }


  # changing NA values into zeroes
  corpus_word_matrix[is.na(corpus_word_matrix)] = 0
  # Creating column that gives the number of documents that a word appears across the corpus
  corpus_word_matrix$df = apply(corpus_word_matrix[,c(2:ncol(corpus_word_matrix))], 1, function(x){sum(x > 0)})

#### ----------------------------------------------------------------------------------------------- ####



#### ----------------------------------------------------------------------------------------------- ####

  #### Calculating TF-IDF  ####
  # For each word we have the frequency that they appear in each Task View text.

  # Getting Term frequencies, number of times word occurs divided by number of unique words in each View
  TF = data.frame(word = corpus_word_matrix$word, t(apply(corpus_word_matrix[,c(2:(length(TaskViews) + 1))], 1, function(x){x/n})))

  # calculating IDF.
  # Calculated for each word in the corpus by taking the log of the number of task views divided by the number of Task Views that the word appears.
  idf = log(base = 2, length(TaskViews)/(corpus_word_matrix$df))

  # Multiply the term frequencies of each word in the Task Views by the idf term for that word.
  TF_IDF = data.frame(word = corpus_word_matrix$word, TF[2:(length(TaskViews) + 1)]*t(idf))

#### ----------------------------------------------------------------------------------------------- ####



#### ----------------------------------------------------------------------------------------------- ####
  #### Individual packages to Task Views ####

  # This code takes the titles and description texts of each package,
  # converts to TF vector then takes cosine similarity with TF-IDF Task View vectors.

  # This function accesses package descriptions and titles. Dirk Eddilbettel code
  # It takes the most up to date information from CRAN
  getPackagesWithTitle <- function() {
    contrib.url(getOption("repos")["CRAN"], "source")
    description <- sprintf("%s/web/packages/packages.rds",
                           getOption("repos")["CRAN"])
    con <- if(substring(description, 1L, 7L) == "file://") {
      file(description, "rb")
    } else {
      url(description, "rb")
    }
    on.exit(close(con))
    db <- readRDS(gzcon(con))
    rownames(db) <- NULL
    db[, c("Package", "Title", "Description")]
  }


  # Using function, get an object with descriptions and titles of all packages
  titles_descriptions_packages_data = getPackagesWithTitle()

  # number of packages in extracted data
  #length(titles_descriptions_packages_data[,"Package"])

  # Removing duplicated packages
  titles_descriptions_packages_data = titles_descriptions_packages_data[!duplicated(titles_descriptions_packages_data[,"Package"]), ]
  # length(titles_descriptions_packages_data[,"Package"])


  ############ TESTING ###########
  # Limits number of observations in dataset to speed up tests
  if(input_CRAN_data$TEST){

    titles_descriptions_packages_data = titles_descriptions_packages_data[titles_descriptions_packages_data[,"Package"] %in% input_CRAN_data$all_CRAN_pks,]

  }
  ################################




  # Creating dataframe object, Titles and Description columns
  titles_descriptions_packages = data.frame(Package = titles_descriptions_packages_data[,"Package"],
                                            text = paste(Title = titles_descriptions_packages_data[,"Title"],
                                                         Description = titles_descriptions_packages_data[,"Description"]))


  # converting to list
  titles_descriptions_packages_ls = as.list(titles_descriptions_packages$text)
  names(titles_descriptions_packages_ls) = titles_descriptions_packages_data[,"Package"]

  # Quick text cleaning
  titles_descriptions_packages_ls_cln = lapply(titles_descriptions_packages_ls, function(x){gsub(x, pattern = "[\n]", replacement = " ")})
  titles_descriptions_packages_ls_cln = lapply(titles_descriptions_packages_ls_cln, function(x){gsub(x, pattern = "<[^>]+>", replacement = " ")})

#### ----------------------------------------------------------------------------------------------- ####



#### ----------------------------------------------------------------------------------------------- ####

  # # To take cosine similarity of a package with Task View I need to convert the package vector to same length of Task View vector.
  # word_selector = TF_IDF[,"word", drop = FALSE]

  # cleaning and converting package text to term frequencies
  fun1 = function(x){
    text_ls_cln = dplyr::tibble(txt = x)
    text_ls_cln = tidytext::unnest_tokens(text_ls_cln, word, txt)
    text_ls_cln$word = textstem::lemmatize_words(text_ls_cln$word)
    suppressWarnings({
    text_ls_cln = data.frame(word = text_ls_cln$word[is.na(as.numeric(text_ls_cln$word))])
    })
    text_ls_cln = dplyr::count(text_ls_cln, word, sort = TRUE, name = "test")
    return(text_ls_cln)
  }

  message("cleaning and converting package text to term frequencies")
  titles_descriptions_packages_freq = pbapply::pblapply(titles_descriptions_packages_ls_cln, fun1)


  # Merging package vectors with Task View vectors and then taking cosine similarity
  fun2 = function(x){

    pkg_tsk_text_comb = merge(x = x, y = TF_IDF, by = "word", all.y = TRUE)
    pkg_tsk_text_comb[is.na(pkg_tsk_text_comb)] = 0

    # Have included here weighting the package vectors by IDF
    pkg_tsk_text_comb$test = pkg_tsk_text_comb$test*(idf)


    cosine = lsa::cosine(as.vector(pkg_tsk_text_comb[,2]), as.matrix(pkg_tsk_text_comb[,-c(1,2)]))
    cosine = cosine[1,-1]

    return(cosine)

  }



  message("Merging package vectors with Task View vectors and then taking cosine similarity")
  titles_descriptions_packages_cosine = pbapply::pblapply(titles_descriptions_packages_freq, fun2)


  feature_matrix_titles_descriptions_packages_cosine = titles_descriptions_packages_cosine

#### ----------------------------------------------------------------------------------------------- ####




  # Creating object to be returned. Which is a list made up of objects needed upstream
  list_to_return = list("feature_matrix_titles_descriptions_packages_cosine" = feature_matrix_titles_descriptions_packages_cosine, "input_CRAN_data" = input_CRAN_data)


  CTVsuggestTrain:::save_or_return_objects(TEST = TEST, list_to_return = list_to_return, limiting_n_observations = limiting_n_observations,
                                           save_output = save_output, save_path = save_path, file_name = file_name)


}
