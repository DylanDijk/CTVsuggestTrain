raw_cln = raw_cln[!grepl(x = raw_cln , pattern = "##")]
raw_cln = stringr::str_squish(raw_cln)
raw_cln = paste(raw_cln, collapse = " ")
raw_cln = gsub(x = raw_cln, pattern = "\\([^()]*\\)", "")
TaskView_sources_text[[TaskView]] = list(YAML = YAML, sections = sections, intro = intro, clean = raw_cln)
}
n = vector(length = length(TaskViews))
names(n) = TaskViews
first = TaskViews[1]
for(i in TaskViews){
# i= TaskViews[1]
# i= TaskViews[2]
text = paste(TaskView_sources_text[[i]]$clean, collapse = " ")
TaskViews_txt = dplyr::tibble(txt = text)
# unnest_tokens converts the character string to separate words
TaskViews_txt = tidytext::unnest_tokens(TaskViews_txt, word, txt)
# absorbs words into lemma word
TaskViews_txt$word = textstem::lemmatize_words(TaskViews_txt$word)
# remove numbers
TaskViews_txt = data.frame(word = TaskViews_txt$word[is.na(as.numeric(TaskViews_txt$word))])
# count number of times each word appears
TaskViews_txt = dplyr::count(TaskViews_txt, word, sort = TRUE, name = i)
# this is the number of unique words in each document
n[i] = nrow(TaskViews_txt)
if(i == first){
corpus_word_matrix = TaskViews_txt
}else{
corpus_word_matrix = merge(x = corpus_word_matrix, y = TaskViews_txt, by = "word", all = TRUE)
}
}
# changing NA values into zeroes
corpus_word_matrix[is.na(corpus_word_matrix)] = 0
# Creating column that gives the number of documents that a word appears across the corpus
corpus_word_matrix$df = apply(corpus_word_matrix[,c(2:ncol(corpus_word_matrix))], 1, function(x){sum(x > 0)})
corpus_word_matrix$df
#### Calculating TF-IDF  ####
# Getting Term frequencies, number of times word occurs divided by number of unique words in each View
TF = data.frame(word = corpus_word_matrix$word, t(apply(corpus_word_matrix[,c(2:(length(TaskViews) + 1))], 1, function(x){x/n})))
TF
corpus_word_matrix
# calcualting IDF
idf = log(base = 2, length(TaskViews)/(corpus_word_matrix$df))
TF_IDF = data.frame(word = corpus_word_matrix$word, TF[2:(length(TaskViews) + 1)]*t(idf))
TF_IDF
idf
corpus_word_matrix
devtools::document()
??usethis
??CTVsuggest
devtools::document()
??CTVsuggest
devtools::load_all()
??CTVsuggest
??CTVsuggest
help(CTVsuggest)
help(CTVsuggest:::get_data)
library(devtools)
document()
help(CTVsuggest:::get_data)
document()
help(CTVsuggest:::get_data)
help(CTVsuggest:::get_data)
document()
help(CTVsuggest:::get_data)
corpus_word_matrix
input_CRAN_data = CTVsuggest:::get_data(TEST = TEST, limiting_n_observations = limiting_n_observations)
TEST = TRUE
limiting_n_observations = 10
input_CRAN_data = CTVsuggest:::get_data(TEST = TEST, limiting_n_observations = limiting_n_observations)
# This script creates the NLP features for the model using Task View text and Package description text.
message("Creating the NLP features for the model using Task View text and Package description text")
#### ----------------------------------------------------------------------------------------------- ####
#### ----------------------------------------------------------------------------------------------- ####
##### Reading and Cleaning Text for all Task Views  #####
message("Reading and Cleaning Text for all Task Views ")
TaskViews = RWsearch::tvdb_vec(input_CRAN_data$tvdb)
TaskView_sources_text = vector(length = length(TaskViews), mode = "list")
names(TaskView_sources_text) = TaskViews
for(TaskView in TaskViews){
print(TaskView)
#TaskView = "WebTechnologies"
#TaskView = "Databases"
#TaskView = "OfficialStatistics"
#TaskView = "MachineLearning"
#TaskView = "HighPerformanceComputing"
#TaskView = "ReproducibleResearch"
if(TaskView == "HighPerformanceComputing"){
raw = readLines(paste0("https://raw.githubusercontent.com/cran-task-views/",TaskView,"/master/",TaskView,".md"))
} else{
raw = readLines(paste0("https://raw.githubusercontent.com/cran-task-views/",TaskView,"/main/",TaskView,".md"))
}
YAML = raw[(min(which(raw == "---")) + 1):(max(which(raw == "---")) - 1)]
#TaskView_references = gsub(x = raw_cln, pattern = "`r [^`]*`", " ")
raw_cln =  raw[(max(which(raw == "---")) + 1):length(raw)]
if(any(grepl(pattern = "# Links", x = raw_cln))){
raw_cln =  raw_cln[1:(which(grepl(pattern = "# Links", x = raw_cln)) - 1)]
}
sections = raw_cln[grepl(x = raw_cln , pattern = "#")]
if(TaskView == "ReproducibleResearch"){
positions_of_sections_ReproducibleResearch = which(grepl(x = raw_cln , pattern = "===") | grepl(x = raw_cln , pattern = "---"))
positions_of_sections_ReproducibleResearch = positions_of_sections_ReproducibleResearch - 1
sections = raw_cln[positions_of_sections_ReproducibleResearch]
}
raw_cln = gsub(x = raw_cln, pattern = "`r[^`]*`", " ")
raw_cln = gsub(x = raw_cln, pattern = "`[^`]*`", " ")
raw_cln = gsub(x = raw_cln, pattern = "\\(https:[^()]*\\)", "")
raw_cln = gsub(x = raw_cln, pattern = "\\(http:[^()]*\\)", "")
raw_cln = gsub(x = raw_cln, pattern = "\\(http:.*", "")
raw_cln = gsub(x = raw_cln, pattern = "\\(https:.*", "")
raw_cln = gsub(x = raw_cln, pattern = "\\([^()]*\\)", "")
# Some Task View text does not have separate topics for example the MachineLearning Task View
if(any(grepl(pattern = "#", x = raw_cln))){
intro = raw_cln[1:(min(which(grepl(raw_cln, pattern ="#"))) - 1)]
} else {
intro = raw_cln[1:(min(which(grepl(raw_cln, pattern ="\\*"))) - 1)]
}
intro = stringr::str_squish(intro)
intro = paste(intro, collapse = " ")
raw_cln = raw_cln[!grepl(x = raw_cln , pattern = "##")]
raw_cln = stringr::str_squish(raw_cln)
raw_cln = paste(raw_cln, collapse = " ")
raw_cln = gsub(x = raw_cln, pattern = "\\([^()]*\\)", "")
TaskView_sources_text[[TaskView]] = list(YAML = YAML, sections = sections, intro = intro, clean = raw_cln)
}
#### ----------------------------------------------------------------------------------------------- ####
#### ----------------------------------------------------------------------------------------------- ####
##### Creates data frame object that gives the count of each word in each Task View ######
n = vector(length = length(TaskViews))
names(n) = TaskViews
first = TaskViews[1]
for(i in TaskViews){
# i= TaskViews[1]
# i= TaskViews[2]
text = paste(TaskView_sources_text[[i]]$clean, collapse = " ")
TaskViews_txt = dplyr::tibble(txt = text)
# unnest_tokens converts the character string to separate words
TaskViews_txt = tidytext::unnest_tokens(TaskViews_txt, word, txt)
# absorbs words into lemma word
TaskViews_txt$word = textstem::lemmatize_words(TaskViews_txt$word)
# remove numbers
TaskViews_txt = data.frame(word = TaskViews_txt$word[is.na(as.numeric(TaskViews_txt$word))])
# count number of times each word appears
TaskViews_txt = dplyr::count(TaskViews_txt, word, sort = TRUE, name = i)
# this is the number of unique words in each document
n[i] = nrow(TaskViews_txt)
if(i == first){
corpus_word_matrix = TaskViews_txt
}else{
corpus_word_matrix = merge(x = corpus_word_matrix, y = TaskViews_txt, by = "word", all = TRUE)
}
}
# changing NA values into zeroes
corpus_word_matrix[is.na(corpus_word_matrix)] = 0
# Creating column that gives the number of documents that a word appears across the corpus
corpus_word_matrix$df = apply(corpus_word_matrix[,c(2:ncol(corpus_word_matrix))], 1, function(x){sum(x > 0)})
# Getting Term frequencies, number of times word occurs divided by number of unique words in each View
TF = data.frame(word = corpus_word_matrix$word, t(apply(corpus_word_matrix[,c(2:(length(TaskViews) + 1))], 1, function(x){x/n})))
TF
corpus_word_matrix
# calculating IDF.
# Calculated for each word in the corpus by taking the log of the number of task views divided by the number of Task Views that the word appears.
idf = log(base = 2, length(TaskViews)/(corpus_word_matrix$df))
idf
corpus_word_matrix$df
corpus_word_matrix
# Multiply the term frequencies of each word in the Task Views by the idf term for that word.
TF_IDF = data.frame(word = corpus_word_matrix$word, TF[2:(length(TaskViews) + 1)]*t(idf))
# This function accesses package descriptions and titles. Dirk Eddilbettel code
# It takes the most up to date information from CRAN
getPackagesWithTitle <- function() {
contrib.url(getOption("repos")["CRAN"], "source")
description <- sprintf("%s/web/packages/packages.rds",
getOption("repos")["CRAN"])
con <- if(substring(description, 1L, 7L) == "file://") {
file(description, "rb")
} else {
url(description, "rb")
}
on.exit(close(con))
db <- readRDS(gzcon(con))
rownames(db) <- NULL
db[, c("Package", "Title", "Description")]
}
# Using function, get an object with descriptions and titles of all packages
titles_descriptions_packages_data = getPackagesWithTitle()
# Removing duplicated packages
titles_descriptions_packages_data = titles_descriptions_packages_data[!duplicated(titles_descriptions_packages_data[,"Package"]), ]
############ TESTING ###########
# Limits number of observations in dataset to speed up tests
if(input_CRAN_data$TEST){
titles_descriptions_packages_data = titles_descriptions_packages_data[titles_descriptions_packages_data[,"Package"] %in% input_CRAN_data$all_CRAN_pks,]
}
# Creating dataframe object, Titles and Description columns
titles_descriptions_packages = data.frame(Package = titles_descriptions_packages_data[,"Package"],
text = paste(Title = titles_descriptions_packages_data[,"Title"],
Description = titles_descriptions_packages_data[,"Description"]))
# converting to list
titles_descriptions_packages_ls = as.list(titles_descriptions_packages$text)
names(titles_descriptions_packages_ls) = titles_descriptions_packages_data[,"Package"]
# Quick text cleaning
titles_descriptions_packages_ls_cln = lapply(titles_descriptions_packages_ls, function(x){gsub(x, pattern = "[\n]", replacement = " ")})
titles_descriptions_packages_ls_cln = lapply(titles_descriptions_packages_ls_cln, function(x){gsub(x, pattern = "<[^>]+>", replacement = " ")})
# cleaning and converting package text to term frequencies
fun1 = function(x){
text_ls_cln = dplyr::tibble(txt = x)
text_ls_cln = tidytext::unnest_tokens(text_ls_cln, word, txt)
text_ls_cln$word = textstem::lemmatize_words(text_ls_cln$word)
text_ls_cln = data.frame(word = text_ls_cln$word[is.na(as.numeric(text_ls_cln$word))])
text_ls_cln = dplyr::count(text_ls_cln, word, sort = TRUE, name = "test")
return(text_ls_cln)
}
message("cleaning and converting package text to term frequencies")
titles_descriptions_packages_freq = pbapply::pblapply(titles_descriptions_packages_ls_cln, fun1)
titles_descriptions_packages_ls_cln
x = titles_descriptions_packages_ls_cln[[1]]
x
text_ls_cln = dplyr::tibble(txt = x)
text_ls_cln
text_ls_cln = tidytext::unnest_tokens(text_ls_cln, word, txt)
text_ls_cln
text_ls_cln$word = textstem::lemmatize_words(text_ls_cln$word)
text_ls_cln
text_ls_cln = dplyr::tibble(txt = x)
text_ls_cln = tidytext::unnest_tokens(text_ls_cln, word, txt)
text_ls_cln
text_ls_cln$word = textstem::lemmatize_words(text_ls_cln$word)
as.numeric(text_ls_cln$word)
is.na(as.numeric(text_ls_cln$word))
is.numeric(text_ls_cln$word)
text_ls_cln = dplyr::tibble(txt = x)
text_ls_cln = tidytext::unnest_tokens(text_ls_cln, word, txt)
text_ls_cln$word = textstem::lemmatize_words(text_ls_cln$word)
suppressWarnings(
text_ls_cln = data.frame(word = text_ls_cln$word[is.na(as.numeric(text_ls_cln$word))])
)
text_ls_cln = dplyr::tibble(txt = x)
text_ls_cln = tidytext::unnest_tokens(text_ls_cln, word, txt)
text_ls_cln$word = textstem::lemmatize_words(text_ls_cln$word)
suppressWarnings({
text_ls_cln = data.frame(word = text_ls_cln$word[is.na(as.numeric(text_ls_cln$word))])
})
text_ls_cln
text_ls_cln = dplyr::count(text_ls_cln, word, sort = TRUE, name = "test")
# cleaning and converting package text to term frequencies
fun1 = function(x){
text_ls_cln = dplyr::tibble(txt = x)
text_ls_cln = tidytext::unnest_tokens(text_ls_cln, word, txt)
text_ls_cln$word = textstem::lemmatize_words(text_ls_cln$word)
suppressWarnings({
text_ls_cln = data.frame(word = text_ls_cln$word[is.na(as.numeric(text_ls_cln$word))])
})
text_ls_cln = dplyr::count(text_ls_cln, word, sort = TRUE, name = "test")
return(text_ls_cln)
}
message("cleaning and converting package text to term frequencies")
titles_descriptions_packages_freq = pbapply::pblapply(titles_descriptions_packages_ls_cln, fun1)
# Merging package vectors with Task View vectors and then taking cosine similarity
fun2 = function(x){
pkg_tsk_text_comb = merge(x = x, y = TF_IDF, by = "word", all.y = TRUE)
pkg_tsk_text_comb[is.na(pkg_tsk_text_comb)] = 0
# Have included here weighting the package vectors by IDF
pkg_tsk_text_comb$test = pkg_tsk_text_comb$test*(idf)
cosine = lsa::cosine(as.matrix(pkg_tsk_text_comb[,-1]))
cosine = cosine[1,-1]
return(cosine)
}
text_ls_cln = dplyr::tibble(txt = x)
text_ls_cln = tidytext::unnest_tokens(text_ls_cln, word, txt)
text_ls_cln$word = textstem::lemmatize_words(text_ls_cln$word)
suppressWarnings({
text_ls_cln = data.frame(word = text_ls_cln$word[is.na(as.numeric(text_ls_cln$word))])
})
text_ls_cln = dplyr::count(text_ls_cln, word, sort = TRUE, name = "test")
text_ls_cln
idf
titles_descriptions_packages_freq = pbapply::pblapply(titles_descriptions_packages_ls_cln, fun1)
titles_descriptions_packages_freq
TF_IDF
x = titles_descriptions_packages_freq[[1]]
x
pkg_tsk_text_comb = merge(x = x, y = TF_IDF, by = "word", all.y = TRUE)
pkg_tsk_text_comb
titles_descriptions_packages_freq = pbapply::pblapply(titles_descriptions_packages_ls_cln, fun1)
# Merging package vectors with Task View vectors and then taking cosine similarity
fun2 = function(x){
pkg_tsk_text_comb = merge(x = x, y = TF_IDF, by = "word", all.y = TRUE)
pkg_tsk_text_comb[is.na(pkg_tsk_text_comb)] = 0
# Have included here weighting the package vectors by IDF
pkg_tsk_text_comb$test = pkg_tsk_text_comb$test*(idf)
cosine = lsa::cosine(as.matrix(pkg_tsk_text_comb[,-1]))
cosine = cosine[1,-1]
return(cosine)
}
message("Merging package vectors with Task View vectors and then taking cosine similarity")
titles_descriptions_packages_cosine = pbapply::pblapply(titles_descriptions_packages_freq, fun2)
feature_matrix_titles_descriptions_packages_cosine = titles_descriptions_packages_cosine
feature_matrix_titles_descriptions_packages_cosine
typeof(feature_matrix_titles_descriptions_packages_cosine)
get_NLP_output = CTVsuggest:::get_NLP(TEST = TEST, limiting_n_observations = limiting_n_observations)
TEST = TRUE
limiting_n_observations = 100
get_NLP_output = CTVsuggest:::get_NLP(TEST = TEST, limiting_n_observations = limiting_n_observations)
input_CRAN_data = get_NLP_output$input_CRAN_data
tvdb = input_CRAN_data$tvdb
input_CRAN_data$CRAN_cranly_data$package
# Identifying packages with no authors
index_of_no_authors = which(unlist(lapply(input_CRAN_data$CRAN_cranly_data$author, length)) == 0)
# replacing with maintainers
if(length(which(unlist(lapply(input_CRAN_data$CRAN_cranly_data$author, length)) == 0)) > 0){
for(i in 1:length(which(unlist(lapply(input_CRAN_data$CRAN_cranly_data$author, length)) == 0))){
#i = 1
input_CRAN_data$CRAN_cranly_data$author[[index_of_no_authors[i]]] = input_CRAN_data$CRAN_cranly_data$maintainer[index_of_no_authors[i]]
}
}
##### Building author and package networks ####
# Note that if running a test and have restricted the number of packages,
# when building the package network the number of packages listed in the node set will increase.
# Because we are looking at all dependencies of these packages.
# Building the package network will also add packages that are not hosted on CRAN but are hosted on other repos.
aut_network <- cranly::build_network(input_CRAN_data$CRAN_cranly_data, perspective = 'author')
pac_network <- cranly::build_network(input_CRAN_data$CRAN_cranly_data, perspective = 'package')
input_CRAN_data$CRAN_cranly_data
cranly::build_network(input_CRAN_data$CRAN_cranly_data, perspective = 'package')
get_NLP_output = CTVsuggest:::get_NLP(TEST = TEST, limiting_n_observations = limiting_n_observations)
input_CRAN_data = get_NLP_output$input_CRAN_data
tvdb = input_CRAN_data$tvdb
#### Objects needed to run generated by previous script ####
# tvdb
# CRAN_data
# CRAN_cranly_data
# all_CRAN_pks
# feature_matrix_titles_descriptions_packages_cosine
##### Objects Outputted ####
# response_matrix
# features
# All_data
# pac_network_igraph
#### ----------------------------------------------------------------------------------------------- ####
#### Replacing missing authors with maintainers ####
# For some packages on CRAN the authors have not been listed in the standard way.
# Which causes these packages to have zero authors listed.
# The work around I have used is by setting the maintainer as the author
# Identifying packages with no authors
index_of_no_authors = which(unlist(lapply(input_CRAN_data$CRAN_cranly_data$author, length)) == 0)
# replacing with maintainers
if(length(which(unlist(lapply(input_CRAN_data$CRAN_cranly_data$author, length)) == 0)) > 0){
for(i in 1:length(which(unlist(lapply(input_CRAN_data$CRAN_cranly_data$author, length)) == 0))){
#i = 1
input_CRAN_data$CRAN_cranly_data$author[[index_of_no_authors[i]]] = input_CRAN_data$CRAN_cranly_data$maintainer[index_of_no_authors[i]]
}
}
#### ----------------------------------------------------------------------------------------------- ####
##### Building author and package networks ####
# Note that if running a test and have restricted the number of packages,
# when building the package network the number of packages listed in the node set will increase.
# Because we are looking at all dependencies of these packages.
# Building the package network will also add packages that are not hosted on CRAN but are hosted on other repos.
aut_network <- cranly::build_network(input_CRAN_data$CRAN_cranly_data, perspective = 'author')
pac_network <- cranly::build_network(input_CRAN_data$CRAN_cranly_data, perspective = 'package')
All_data = list("aut_network" = aut_network, "pac_network" = pac_network)
# All_data_igraph = as.igraph(All_data$pac_network)
pac_network_igraph = igraph::as.igraph(All_data$pac_network)
devtools::load_all()
CTVsuggest::Train_model(TEST = T, limiting_n_observations = 4000, save = T)
install.packages(devtools)
install.packages("devtools")
sessionInfo()
sessionInfo()
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
install.packages("devtools")
library(devtools)
devtools::load_all()
devtools::load_all()
library(glmnet)
age     <- c(4, 8, 7, 12, 6, 9, 10, 14, 7)
gender  <- as.factor(c(1, 0, 1, 1, 1, 0, 1, 0, 0))
bmi_p   <- c(0.86, 0.45, 0.99, 0.84, 0.85, 0.67, 0.91, 0.29, 0.88)
m_edu   <- as.factor(c(0, 1, 1, 2, 2, 3, 2, 0, 1))
p_edu   <- as.factor(c(0, 2, 2, 2, 2, 3, 2, 0, 0))
f_color <- as.factor(c("blue", "blue", "yellow", "red", "red", "yellow",
"yellow", "red", "yellow"))
asthma <- c(1, 1, 0, 1, 0, 0, 0, 1, 1)
xfactors <- model.matrix(asthma ~ gender + m_edu + p_edu + f_color)[, -1]
x        <- as.matrix(data.frame(age, bmi_p, xfactors))
# Note alpha=1 for lasso only and can blend with ridge penalty down to
# alpha=0 ridge only.
glmmod <- glmnet(x, y=as.factor(asthma), alpha=1, family="binomial")
library(Matrix)
train_sparse <- Matrix::sparse.model.matrix(~., as.data.frame(train_features))
library(Matrix)
train_sparse <- Matrix::sparse.model.matrix(~., as.data.frame(x))
train_res_sparse <- Matrix::sparse.model.matrix(~0 + ., as.data.frame(x))
# Note alpha=1 for lasso only and can blend with ridge penalty down to
# alpha=0 ridge only.
glmmod <- glmnet(x, y=as.factor(asthma), alpha=1, family="binomial")
glmmod
library(glmnet)
age     <- c(4, 8, 7, 12, 6, 9, 10, 14, 7)
gender  <- as.factor(c(1, 0, 1, 1, 1, 0, 1, 0, 0))
bmi_p   <- c(0.86, 0.45, 0.99, 0.84, 0.85, 0.67, 0.91, 0.29, 0.88)
m_edu   <- as.factor(c(0, 1, 1, 2, 2, 3, 2, 0, 1))
p_edu   <- as.factor(c(0, 2, 2, 2, 2, 3, 2, 0, 0))
f_color <- as.factor(c("blue", "blue", "yellow", "red", "red", "yellow",
"yellow", "red", "yellow"))
asthma <- c(1, 1, 0, 1, 0, 0, 0, 1, 1)
xfactors <- model.matrix(asthma ~ gender + m_edu + p_edu + f_color)[, -1]
x        <- as.matrix(data.frame(age, bmi_p, xfactors))
library(devtools)
load_all()
help(CTVsuggestTrain:::get_data())
help(CTVsuggestTrain:::get_data
)
document()
?CTVsuggestTrain:::get_data()
?str_unique
??str_unique
library(stringr)
?str_unique
??str_unique
CTVsuggestTrain:::download_taskview_data()
tvdb = CTVsuggestTrain:::download_taskview_data()
tvdb$Pharmacokinetics
tvdb$Pharmacokinetics$name
tvdb$Pharmacokinetics$packagelist
tvdb$Bayesian$packagelist
?cranly
document()
document()
?CTVsuggestTrain:::get_data
??CTVsuggestTrain:::get_data
?get_data
document()
?get_data
document()
?get_data
document()
?get_data
document()
?get_data
knitr::opts_chunk$set(echo = TRUE)
library(profvis)
load_all()
profvis(CTVsuggestTrain:::get_data())
getwd()
devtools::document()
knitr::opts_chunk$set(echo = TRUE)
devtools::load_all()
devtools::load_all()
profvis(CTVsuggestTrain:::get_data())
library(profvis)
profvis(CTVsuggestTrain:::get_data())
getwd()
install.packages("widgetframe")
devtools::load_all()
library(profvis)
profvis(CTVsuggestTrain:::get_data())
profvis(CTVsuggestTrain:::get_NLP())
profvis(
get_NLP(get_input_stored = TRUE, save_output = TRUE,
get_input_path = "tests/testthat/fixtures/get_data_output/get_data_output_test_1000.rds",
file_name = "get_NLP_output_test_1000.rds")
)
devtools::build_readme()
devtools::build_readme()
get_CRAN_logs_output = CTVsuggestTrain:::get_CRAN_logs()
# Loading vector of packages with no Task View assignment that do not meet threshold
no_tsk_pckgs_meet_threshold = base::intersect(get_CRAN_logs_output$no_tsk_pckgs_meet_threshold, get_CRAN_logs_output$final_package_names)
no_tsk_pckgs_meet_threshold = unique(no_tsk_pckgs_meet_threshold)
# Combining the two sets
labelled_data_res = (rbind(get_CRAN_logs_output$response_matrix[get_CRAN_logs_output$response_matrix[,"none"] == 0,],    get_CRAN_logs_output$response_matrix[no_tsk_pckgs_meet_threshold,]))
labelled_data_features = get_CRAN_logs_output$features[rownames(labelled_data_res),]
# Splitting the response matrix and feature matrix with 80:20 ratio
set.seed(3)
split1<- sample(c(rep(0, 0.8 * nrow(labelled_data_res)), rep(1, 0.2 * nrow(labelled_data_res))))
train_res = labelled_data_res[split1 == 0,]
train_features = labelled_data_features[split1 == 0,]
test_res = labelled_data_res[split1 == 1,]
test_feature = labelled_data_features[split1 == 1,]
train_res = as.matrix(train_res)
train_features = as.matrix(train_features)
train_sparse <- Matrix::sparse.model.matrix(~., as.data.frame(train_features))
train_res_sparse <- Matrix::sparse.model.matrix(~0 + ., as.data.frame(train_res))
message("Training model")
set.seed(3)
model_multinom_cv = glmnet::cv.glmnet(x = train_sparse,  y = train_res, family = "multinomial", alpha = 1, trace.it = 1, nlambda = 200)
model = model_multinom_cv
predict_class = predict(model, newx = cbind(rep(1, nrow(test_feature)),as.matrix(test_feature)), s = "lambda.min",  type = "class")
predict_class
# Getting accuracy of model after applying lasso with min Lambda
predict_class = factor(predict_class[,1], levels = c(RWsearch::tvdb_vec(get_CRAN_logs_output$tvdb), "none"))
predict_class
prop.table(predict_class)
prop.table(table(predict_class))
test_res[cbind(1:nrow(test_res)
test_res
test_res[,"none"]
sum(test_res[,"none"])/length(test_res[,"none"])
test_res[cbind(1:nrow(test_res), predict_class)]
which(test_res[,"none"] == 1)
test_res[cbind(1:nrow(test_res), predict_class)][which(test_res[,"none"] == 1)]
prop(test_res[cbind(1:nrow(test_res), predict_class)][which(test_res[,"none"] == 1)])
mean(test_res[cbind(1:nrow(test_res), predict_class)][which(test_res[,"none"] == 1)])
devtools::check()
